# -*- coding: utf-8 -*-
"""HW6-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SY2P_VSXtkk-gm1LQPS1GPML4JN-KJky
"""

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, optimizers
import os
import shutil

# Step 1: Business Understanding (CRISP-DM Step 1)
# Goal: Classify images as 'with_mask' or 'without_mask' using transfer learning with a pretrained VGG16 model.

# Step 2: Data Understanding (CRISP-DM Step 2)
# Clone the dataset repository from GitHub
repo_dir = "Face-Mask-Detection-"
if os.path.exists(repo_dir):
    shutil.rmtree(repo_dir)  # Remove existing directory to avoid conflicts
!git clone https://github.com/chauhanarpit09/Face-Mask-Detection-.git

# Verify dataset paths
possible_dirs = ["Face-Mask-Detection-/Dataset", "Face-Mask-Detection-/data", "Face-Mask-Detection-/Face Mask Dataset"]
base_dir = None
for dir_option in possible_dirs:
    if os.path.exists(dir_option):
        base_dir = dir_option
        break
if base_dir is None:
    raise FileNotFoundError("Dataset directory not found in known paths. Check the repository structure.")

train_dir = os.path.join(base_dir, 'Train')
val_dir = os.path.join(base_dir, 'Validation')
test_dir = os.path.join(base_dir, 'Test')

if not os.path.exists(train_dir):
    train_dir = os.path.join(base_dir, 'train')
if not os.path.exists(val_dir):
    val_dir = os.path.join(base_dir, 'validation')
if not os.path.exists(test_dir):
    test_dir = os.path.join(base_dir, 'test')

if not os.path.exists(train_dir) or not os.path.exists(val_dir) or not os.path.exists(test_dir):
    raise FileNotFoundError("One or more dataset directories (Train, Validation, Test) are missing.")

# Step 3: Data Preparation (CRISP-DM Step 3)
# Prepare ImageDataGenerators
train_datagen = ImageDataGenerator(rescale=1.0/255.0)
val_datagen = ImageDataGenerator(rescale=1.0/255.0)
test_datagen = ImageDataGenerator(rescale=1.0/255.0)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary')

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary')

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary')

# Step 4: Modeling (CRISP-DM Step 4)
# Load pretrained VGG16 model without the top layer
vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model
vgg16_base.trainable = False

# Build the model
model = models.Sequential([
    vgg16_base,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Step 5: Evaluation (CRISP-DM Step 5)
# Train the model
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator)

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Step 6: Deployment (CRISP-DM Step 6)
def test_image(image_url, model, class_names):
    import requests
    from PIL import Image
    from io import BytesIO

    response = requests.get(image_url)
    img = Image.open(BytesIO(response.content)).resize((224, 224))
    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension

    predictions = model.predict(img_array)
    predicted_class = class_names[int(predictions[0] > 0.5)]
    print(f"Predicted class: {predicted_class}")

# Example Usage
image_url = input("Enter image URL: ")
test_image(image_url, model, ['without_mask', 'with_mask'])

import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Find dataset directory
def find_dataset_dir(base_path):
    for root, dirs, files in os.walk(base_path):
        if 'train' in dirs and 'test' in dirs:
            return root
    return None

# Clone repository if not exists
if not os.path.exists('/content/Face-Mask-Detection-'):
    !git clone https://github.com/chauhanarpit09/Face-Mask-Detection-.git

# Find dataset directory
dataset_base_path = find_dataset_dir('/content/Face-Mask-Detection-')

if dataset_base_path:
    train_dir = os.path.join(dataset_base_path, 'train')
    validation_dir = os.path.join(dataset_base_path, 'test')
else:
    # Fallback to manual path specification
    train_dir = '/content/Face-Mask-Detection-/train'
    validation_dir = '/content/Face-Mask-Detection-/test'

print(f"Train Directory: {train_dir}")
print(f"Validation Directory: {validation_dir}")

# Verify directories exist
print("Train Directory Contents:")
print(os.listdir(train_dir))
print("\nValidation Directory Contents:")
print(os.listdir(validation_dir))

# Data Generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

# Create generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    validation_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

# Face Mask Detection Project Setup and Diagnostics

# 1. Install necessary libraries
!pip install tensorflow keras numpy matplotlib

import os
import sys
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# 2. Clone the repository
!git clone https://github.com/chauhanarpit09/Face-Mask-Detection-.git

# 3. Diagnostic Function to Explore Repository
def explore_repository(base_path):
    print("üîç Repository Structure Exploration:")
    for root, dirs, files in os.walk(base_path):
        level = root.replace(base_path, '').count(os.sep)
        indent = ' ' * 4 * level
        print(f"{indent}{os.path.basename(root)}/")
        subindent = ' ' * 4 * (level + 1)
        for dir in dirs:
            print(f"{subindent}{dir}/")
        for file in files[:5]:  # Limit to first 5 files
            print(f"{subindent}{file}")

# 4. Find Dataset Directory
def find_dataset_directories(base_path):
    dataset_dirs = []
    for root, dirs, files in os.walk(base_path):
        if 'train' in dirs or 'test' in dirs:
            dataset_dirs.append(root)
    return dataset_dirs

# 5. Repository Exploration
print("üìÇ Exploring Repository Directories:")
explore_repository('/content/Face-Mask-Detection-')

# 6. Find Potential Dataset Directories
potential_dataset_paths = find_dataset_directories('/content/Face-Mask-Detection-')
print("\nüìä Potential Dataset Paths:")
for path in potential_dataset_paths:
    print(path)

# 7. Attempt to Locate Train and Test Directories
def locate_train_test_dirs(base_path):
    train_dirs = []
    test_dirs = []
    for root, dirs, files in os.walk(base_path):
        if 'train' in dirs:
            train_dirs.append(os.path.join(root, 'train'))
        if 'test' in dirs:
            test_dirs.append(os.path.join(root, 'test'))
    return train_dirs, test_dirs

train_directories, test_directories = locate_train_test_dirs('/content/Face-Mask-Detection-')

print("\nüî¨ Detected Train Directories:")
for dir in train_directories:
    print(dir)
    print("  Contents:", os.listdir(dir)[:5])

print("\nüî¨ Detected Test Directories:")
for dir in test_directories:
    print(dir)
    print("  Contents:", os.listdir(dir)[:5])

# If multiple paths found, we'll use the first valid path
train_dir = train_directories[0] if train_directories else None
test_dir = test_directories[0] if test_directories else None

# 8. Data Generator Setup (Flexible)
if train_dir and test_dir:
    print("\nüöÄ Setting Up Data Generators...")
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        validation_split=0.2
    )

    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(224, 224),
        batch_size=32,
        class_mode='binary',
        subset='training'
    )

    validation_generator = train_datagen.flow_from_directory(
        test_dir,
        target_size=(224, 224),
        batch_size=32,
        class_mode='binary',
        subset='validation'
    )

    # 9. Model Building
    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    output = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=base_model.input, outputs=output)

    # 10. Model Compilation
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # 11. Model Training
    print("\nüèãÔ∏è Training Model...")
    history = model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // train_generator.batch_size,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // validation_generator.batch_size,
        epochs=5
    )

    # 12. Model Evaluation
    print("\nüìä Model Evaluation:")
    evaluation = model.evaluate(validation_generator)
    print(f"Test Loss: {evaluation[0]}")
    print(f"Test Accuracy: {evaluation[1]}")
else:
    print("‚ùå Could not locate train and test directories!")